# 音素 phone

音素是语音的最小单位。

音素是根据语音的自然属性划分出来的最小语音单位。从声学性质来看，音素是从音质角度划分出来的最小语音单位。从生理性质来看，一个发音动作形成一个音素。如〔ma〕包含〔m〕〔a〕两个发音动作，是两个音素。

**音素分类**：元音和辅音

# 音标

记录音素的符号叫做音标，我们常说的英语中48个音标，实际上应该是48音素。

# 音节 syllable

音节指语音学上的基本单位，是由一个或多个音素组成的音段（一般是由单个元音音素和辅音音素组合而成的语音单位，有时候单个元音音素也可自成音节）。

音节具有较为明显的感知界限。 比如一个汉字对应一个音节，一个假名也对应一个音节。而单词wa·ter则有两个音节。

# 共振峰 formant

共振峰是反映声道谐振特性的重要特征，它代表了发音信息的最直接的来源，而且人在语音感知中利用了共振峰信息。所以共振峰是语音信号处理中非常重要的特征参数。

共振峰在语谱图上显示为**水平方向的亮色带**，而在线性频谱(spectrum)上则体现为**特定频率的振幅峰**。

无论是人声还是乐器，它们的声音特性都源自两个因素，一个是发声系统，如人的声带或乐器的振动簧片，另一个是共鸣系统。

乐器不同的共鸣系统使其在**一定频域**中的分音的**振幅**得以突出，这样，这些区域就产生了这个乐器所特有的共振峰值，这些共振峰值同共鸣体的**大小**、**形状**和**材料**密切相关。

由于一件乐器的结构是稳定的，因此在一件乐器发出的所有音调中，不论基频如何，都会表现出相同的共振峰值，只不过其显著性有强有弱罢了。

这就可以帮助我们解释为什么在很多的乐器中，同一乐器所发出的不同音调具有相同的音质。

一切**元音**都有一个基音，并有至少两个语音加强频带，亦即有两个共振峰;一般用F1、F2等表示。

我们声带振动发出的声音，经过身体里不同的腔体产生共振或者衰减之后，就产生了各种各样的音色。而经过各种腔体之后，被显著增大的那部分频谱，就是共振峰了。

在语音声学中，人声也同样受自身生理如鼻孔、咽腔、口腔大小的影响有自身的共振峰区（Formant Regions）。通过利用这些共鸣空间的**形状和大小不同的变化**（例如改变咽喉、嘴形），我们就能改变声音的共振峰。

我们之所以能够区分不同的人声、元音，主要也是依靠它们的共振峰分布的位置。

# 预加重 pre-emphasis

空气是语音信号的载体，会传播和损耗声波的能量。声源尺寸一定，频率越高，损耗越大。

像元音等一些因素的发音包含了较多的高频信号的成分，高频信号的丢失，可能会导致音素的共振峰并不明显，使得声学模型对这些音素的建模能力不强。

预加重是个一阶高通滤波器，可以提高信号高频部分的能量，给定时域输入信号$`x[n]`$，则$`y[n]=x[n]-\alpha*x[n - 1]`$。

其中，$`\alpha`$一般设置为0.97，取值范围为0.9 - 1

![image](https://github.com/Heterogeneity/Notes/assets/102458836/0f4f3888-d391-496b-9cbc-bf97ad20731b)

# 谱包络 spectral envelope

频谱图中红线表示的特征提取出来，移除蓝色的影响部分。其中红色平滑曲线将各个共振峰连接起来，这条红线，称为谱包络。

![image](https://github.com/Heterogeneity/Notes/assets/102458836/df39e1e5-77fc-4990-8472-a42535986cb3)

# 分帧

语音信号是一个非稳态的、时变的信号。

FFT要求输入信号是平稳的，即信号要么是从$`-\infty`$到$`\infty`$，要么是周期信号。

语音信号短时平稳，对语音进行短时分帧(frame)，以满足FFT平稳条件，一般为10ms到25ms为一帧。

# 频率泄露

FFT的输入信号如果是非周期有限信号，则对输入的帧信号进行延拓重构后，结果会出现两帧连接处出现振幅值不连续的跳跃现象，这导致FFT对该信号的分析结果不再是信号的真实频率，而是会出现“拖尾”现象，
在真实频率周围出现原本不存在的频率，造成极大误差，这种现象叫做频率泄露。

在现实世界中，我们用到的帧信号往往都是非周期的信号。

![image](https://github.com/Heterogeneity/Notes/assets/102458836/f14b54ea-1bb6-4e93-8fb0-e1afabe3ae38)
![image](https://github.com/Heterogeneity/Notes/assets/102458836/d5240e8d-406f-457a-9151-e7ef55b71434)

# 加窗

**分帧**和**频率泄露**中提到FFT处理非周期信号会由于截断效应造成极大误差，为了减小这种误差，我们希望有一种方法能够将延拓的两帧之间跳跃现象“柔化”，即让边界衔接自然一些，使帧信号近似周期信号。

通过将帧信号乘以一个窗函数，我们就可以达到这样的效果。

![image](https://github.com/Heterogeneity/Notes/assets/102458836/7434248d-eb26-4664-a001-05da28394cb9)

# 梅尔刻度 mel

通过实际的主观实验，科学家发现人耳对低频信号的区别更加敏感，而对高频信号的区别则不那么敏感。

也就是说低频段上的两个频度和高频段上的两个频度，人们会更容易区分前者。即频域上相等距离的两对频度，对于人耳来说他们的距离不一定相等。

为了调整频域的刻度，使得这个新的刻度上相等距离的两对频度，对于人耳来说也相等，梅尔刻度诞生了。

![image](https://github.com/Heterogeneity/Notes/assets/102458836/b661802f-c191-425a-a6a9-4465fe40719c)

用梅尔滤波器乘以原始语谱图，得到梅尔频谱

![image](https://github.com/Heterogeneity/Notes/assets/102458836/f565394e-7f43-4b51-adae-c6f504f3f878)


![image](https://github.com/Heterogeneity/Notes/assets/102458836/1eaca8fc-f8dd-4150-86de-f2f21583987c)

这个梅尔频谱的大小 (128 × 359) 是功率谱 (513 × 359) 的1/4，因此在模型训练中使用的话可以大幅减少cost。

既然降维了，那么原本数据的信息必然损失了一部分，但因为梅尔刻度是针对人耳设计的，因此梅尔频谱很大程度上保留了人耳理解原本语音所需的信息，这就是梅尔频谱的精髓所在。

# 采样率

计算机只能处理离散的数据，而音频信号是模拟信号，即音频信号是连续的，为了能够让计算机处理这种连续信号，需要声卡将连续的模拟信号转换为离散的信号，具体来说，是通过每秒对音频信号进行采集样点操作，
用这些点值去拟合原信号。采样率即每秒钟采集的样本点个数，采样率越高，对原始信号的还原度就越高，同时要存储的数据也越多。

![image](https://github.com/Heterogeneity/Notes/assets/102458836/6f406711-d711-4d70-af0e-efa98d059771)

# 比特深度/采样精度

采样率是在时间轴上对原始信号进行采样，而这些样本点的值（即幅值）也是连续的，样本点值的表示精度和动态范围就由比特深度/采样精度决定。比特深度越大，每个样本点能表示的幅值范围就越大，且精度越高。与采样率比较，
比特深度是在纵轴（即振幅轴）上进行分割，而采样率是在横轴（即时间轴）上进行分割，两者共同决定了计算机中的离散值对原始音频信号的还原度。

音频的比特深度决定动态范围。常见的16Bit（16比特），可以记录大概96分贝的动态范围。即每一个比特大约可以记录6分贝的声音。
